{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Необходимый импорт"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import spacy\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from fastai.text.all import *\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0583], requires_grad=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(1).requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1320, 0.6545, 0.5325],\n",
       "        [0.1655, 0.8553, 0.3148],\n",
       "        [0.7108, 0.9380, 0.9344]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5, 6],\n",
       "        [8, 9]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([i for i in range(1,10)]).view(-1,3)[1:3,1:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Открытие файла"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_folder = Path(\"../../SomeData/dataPod\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tec_sup_data = pd.read_csv(path_to_folder/\"data.csv\", sep=\";\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Тема', 'Описание', 'Категория', 'Тех. поддержка'], dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tec_sup_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tec_sup_data[1].iloc[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tec_sup_data = pd.read_excel(path_to_folder/\"data.xlsx\")\n",
    "tec_sup_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tec_sup_data[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подготавливаем данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = tec_sup_data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99354, 4)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(92536, 4)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(tec_sup_data.shape)\n",
    "display(data_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spacy.require_gpu()\n",
    "nlpru = spacy.load(\"ru_core_news_lg\")\n",
    "stopwords = spacy.lang.ru.stop_words.STOP_WORDS\n",
    "punctuations = string.punctuation+\"-\"+\"...\"+\"”\"+\"”\"+\"№\"\n",
    "IP_SITE_REGEX = r\"(?:(?:https?|ftp|http):\\/\\/)?[\\w\\/\\-?=%.]+\\.[\\w\\/\\-&?=%.]+\"#r\"(.*[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b[-a-zA-Z0-9()@:%_\\/+.~#?&=]*)\"\n",
    "MAIL_REGEX = r\"([a-zA-Z0-9._-]+@[a-zA-Z0-9._-]+\\.[a-zA-Z0-9_-]+)\"\n",
    "SCIP_DATA_REGEX = r\"^\\d+|^[Tt][Ee][Ss][Tt]|^[Пп][Рр][Оо][Вв][Ее][Рр][Кк][Аа]\"\n",
    "NUM_REGEX = r\"\\d+(?!\\w)\"#r\"\\d+(?= |\\n|$)\"\n",
    "PUNCT_REGEX = r\"[!\\\"#$%&\\'()*+,-./:;<=>?@\\[\\\\\\]^_`{|}~\\-...№«»„“‟”⹂•]\"\n",
    "NAMED_PAR_REGEX = r\"(?<! )([А-ЯЁA-Z]+[а-яёa-z\\._]+){2,}(?! )\"#r\"(\\w+[_.])+([А-ЯЁ][а-яё]+))\"#r\"(\\w+[_.])|((?<! )([А-ЯЁ][а-яё]+)(?! ))\"\n",
    "#NAMED2_PAR_REGEX = r\"(?<! )([А-ЯЁ][а-яё]+){2,}(?! )\"\n",
    "\n",
    "\n",
    "def cleanup_text_sp(docs):\n",
    "    tokens = nlpru(str(docs))\n",
    "    text = []\n",
    "    for tok in tokens:\n",
    "        if not (re.search(NUM_REGEX, str(tok))) and not (re.search(IP_SITE_REGEX, str(tok)))\\\n",
    "            and not (re.search(r\" {2,}\", str(tok))):\n",
    "            text.append(tok.text)\n",
    "        elif (re.search(NUM_REGEX, str(tok))):\n",
    "            text.append(\"ЧИСЛО\")\n",
    "        elif (re.search(IP_SITE_REGEX, str(tok))):\n",
    "            text.append(\"АДРСАЙТА\")\n",
    "        elif (re.search(r\" {2,}\", str(tok))):\n",
    "            continue\n",
    "    #print(text)\n",
    "    tokens = [tok for tok in text if tok not in (stopwords and punctuations)]\n",
    "    #tokens = [tok for tok in tokens if tok not in SYMBOLS]  and not (re.search(IP_SITE_REGEX, str(tok))\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "def cleanup_text(docs:str):\n",
    "    text_list = docs.split()\n",
    "    output_text = []\n",
    "    for item in text_list:\n",
    "        #if re.sub(r'^\\s+|\\s+$', \"\", item.lower()) in stopwords:\n",
    "        #    continue\n",
    "        if not (re.search(NUM_REGEX + \"|\" + PUNCT_REGEX + \"|\" + IP_SITE_REGEX + \"|\" + NAMED_PAR_REGEX, str(item)))\\\n",
    "            and not (re.search(r\" {2,}\", str(item))):\n",
    "            output_text.append(item)\n",
    "            continue\n",
    "        elif (re.search(IP_SITE_REGEX, str(item))):\n",
    "            #tmp = re.sub(IP_SITE_REGEX, \" АДРСАЙТА\", item)\n",
    "            #tmp = re.sub(NUM_REGEX, \" ЧИСЛО\", tmp)\n",
    "            #tmp = re.sub(PUNCT_REGEX, \"\", tmp)\n",
    "            output_text.append(\"АДРСАЙТА\")\n",
    "        elif (re.search(NUM_REGEX, str(item))):\n",
    "            #tmp = re.sub(NUM_REGEX, \"ЧИСЛО\", item)\n",
    "            #tmp = re.sub(PUNCT_REGEX, \"\", tmp)\n",
    "            output_text.append(\"ЧИСЛО\")\n",
    "        elif (re.search(NAMED_PAR_REGEX, str(item))):\n",
    "            output_text.append(\"ИМПАР\")\n",
    "        elif (re.search(PUNCT_REGEX, str(item))):\n",
    "            temp = re.sub(PUNCT_REGEX, \"\", item)\n",
    "            if temp == \"\":\n",
    "                continue\n",
    "            output_text.append(temp)\n",
    "    return \" \".join(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(r'^\\s+|\\s+$', \"\", \"в\".lower()) in stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanup_text(tec_sup_data[\"Описание\"].iloc[65532])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tec_sup_data[\"Описание\"].iloc[65532]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~-...””№'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation+\"-\"+\"...\"+\"”\"+\"”\"+\"№\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ИМПАРМодификацияОбъектов'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(r\"(.+[_.])+\", \"ИМПАР\", \"ИСТОК_МодификацияОбъектов\", count=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_rem = []\n",
    "data_set = tec_sup_data.replace(r'^\\s+|\\s+$','',regex=True).copy()\n",
    "#data_set = data_set[data_set[\"Тех. поддержка\"] == \"ОИТ\"]\n",
    "data_set['Категория'] = data_set['Категория'].astype( str )\n",
    "data_set[\"Описание\"] = data_set[\"Описание\"].astype( str )\n",
    "data_set[\"Описание\"] = data_set[\"Описание\"].apply(lambda x: cleanup_text(x))\n",
    "#data_set[\"Описание\"] = data_set[\"Описание\"].replace(r'^\\s+|\\s+$','',regex=True)\n",
    "#data_set[\"Описание\"] = data_set[\"Описание\"].replace(MAIL_REGEX,'ПОЧТА',regex=True)\n",
    "#data_set[\"Описание\"] = data_set[\"Описание\"].replace(IP_SITE_REGEX,'АДРСАЙТА',regex=True)\n",
    "#data_set[\"Описание\"] = data_set[\"Описание\"].replace(NAMED_PAR_REGEX,\"ИМПАР\",regex=True)\n",
    "#data_set[\"Описание\"] = data_set[\"Описание\"].apply(lambda x: str(x).translate(str.maketrans('', '', string.punctuation+\"№\")))\n",
    "#data_set[\"Описание\"] = data_set[\"Описание\"].replace(NUM_REGEX,'ЧИСЛО',regex=True)\n",
    "#data_set[\"Описание\"] = data_set[\"Описание\"].replace(PUNCT_REGEX,'',regex=True)\n",
    "#data_set[\"Описание\"] = data_set[\"Описание\"].replace(r\" {2,}\",' ',regex=True)\n",
    "for data in data_set[\"Описание\"]:\n",
    "    if str(data)== \"ЧИСЛО\" or re.search(SCIP_DATA_REGEX, str(data)):\n",
    "        for x in data_set[data_set[\"Описание\"] == data].index:\n",
    "            index_rem.append(x)\n",
    "data_set.drop(index_rem, inplace=True)\n",
    "data_set.reset_index(drop=True, inplace=True)\n",
    "index_rem = None\n",
    "#data_set['Категория'] = data_set['Категория'].str.replace('\\\"', '') \n",
    "data_set['Категория'] = data_set['Категория'].str.lower()\n",
    "data_set[\"Описание\"] = data_set[\"Описание\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = pd.read_csv(\"../output/data_set.csv\", sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set.to_csv(path_to_folder/'output/data_set.csv', header=True, sep=';', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanup_text(tec_sup_data[\"Описание\"].iloc[65532])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docRu = nlpru(tec_sup_data[\"Описание\"].iloc[65532])\n",
    "spacy.displacy.render(docRu, style=\"dep\", jupyter= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlpru(tec_sup_data[\"Описание\"].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap_space(\"ФизическиеЛицаЗарплатаКадры\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        50\n",
       "1        50\n",
       "2        47\n",
       "3        47\n",
       "4        47\n",
       "         ..\n",
       "98808    23\n",
       "98809     6\n",
       "98810    48\n",
       "98811    23\n",
       "98812    23\n",
       "Length: 98813, dtype: int8"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set['Категория'] = data_set['Категория'].astype(\"category\")\n",
    "data_set[\"Категория\"].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(88715, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set = data_set.drop([\"Тема\", \"Тех. поддержка\"], axis=1)\n",
    "data_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_cat = data_set['Категория'].value_counts().to_frame().reset_index()\n",
    "drop_name = []\n",
    "print(cnt_cat.shape)\n",
    "for index in cnt_cat.index:\n",
    "    if cnt_cat['count'][index] <= 500:\n",
    "        drop_name.append(cnt_cat['Категория'][index])\n",
    "print(drop_name)\n",
    "for item in drop_name:\n",
    "    data_set = data_set[data_set[\"Категория\"] != item]\n",
    "print(data_set[\"Категория\"].value_counts().shape)\n",
    "#for item in drop_name:\n",
    "#    data_set[\"Категория\"][(data_set[\"Категория\"] == item)] = \"Не определено\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_set[\"Категория\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(data_set[\"Категория\"])\n",
    "data_set[\"LableEncoder\"] = le.transform(data_set[\"Категория\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Shape: (52155, 3)\n",
      "Testing Data Shape: (17386, 3)\n",
      "1629\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test = train_test_split(data_set, test_size=0.2, random_state=1)\n",
    "\n",
    "X_train, X_val = train_test_split(X_train, test_size=0.25, random_state=1) # 0.25 x 0.8 = 0.2\n",
    "\n",
    "print('Training Data Shape:', X_train.shape)\n",
    "print('Testing Data Shape:', X_test.shape)\n",
    "print(X_train.shape[0]//32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[X_train[\"LableEncoder\"]==14]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Строим словарь из данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95402"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_set[\"Описание\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создаём датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set[[\"Описание\", \"LableEncoder\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupTextDataset(Dataset):\n",
    "    def __init__(self, tablet:pd.DataFrame, max_lable):\n",
    "        self.max_index_lable = max_lable\n",
    "        self.lable = tablet[\"LableEncoder\"]\n",
    "        self.text = tablet[\"Описание\"]\n",
    "      \n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        example = self.text.iloc[idx]\n",
    "        lable = self.lable.iloc[idx]\n",
    "        return example, int(lable)#tuple(0 if i != lable else 1 for i in range(0, self.max_index_lable))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = iter(SupTextDataset(X_train[[\"Описание\", \"LableEncoder\"]], len(data_set[\"Категория\"].value_counts())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('обновление импар', 8)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('не включается пк', 2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(train_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Определяем tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set_iter = SupTextDataset(data_set[[\"Описание\", \"LableEncoder\"]], len(data_set[\"Категория\"].value_counts()))\n",
    "tokenizer = get_tokenizer(tokenizer=\"spacy\", language=\"ru_core_news_lg\")\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for text, _ in data_iter:\n",
    "        yield tokenizer(cleanup_text(str(text)))\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"], max_tokens=20000)\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "text_pipeline = lambda x: vocab(tokenizer(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(tokenizer=\"spacy\", language=\"ru_core_news_lg\")\n",
    "text_pipeline = lambda x: vocab(tokenizer(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.get_stoi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list, offsets = [], [], [0]\n",
    "    for _text, _label in batch:\n",
    "        label_list.append(_label)\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        offsets.append(processed_text.size(0))\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text_list = torch.cat(text_list)\n",
    "    return label_list.to(device), text_list.to(device), offsets.to(device)\n",
    "\n",
    "\n",
    "train_iter = SupTextDataset(X_train[[\"Описание\", \"LableEncoder\"]], len(data_set[\"Категория\"].value_counts()))\n",
    "dataloader = DataLoader(\n",
    "    train_iter, batch_size=32, shuffle=False, drop_last=True, collate_fn=collate_batch\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создаём модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=False)\n",
    "        self.drop = nn.Dropout()\n",
    "        self.fc0 = nn.Linear(embed_dim, 256)\n",
    "        self.activ = nn.ReLU()\n",
    "        self.drop2 = nn.Dropout()\n",
    "        self.fc1 = nn.Linear(256, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc0.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc0.bias.data.zero_()\n",
    "        self.fc1.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc1.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        x = self.fc0(embedded)\n",
    "        x = self.activ(x)\n",
    "        x = self.drop2(x)\n",
    "        return self.fc1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OurFirstLSTM(nn.Module):\n",
    "    def __init__(self, hidden_size, embedding_dim, vocab_size, num_class):\n",
    "        super(OurFirstLSTM, self).__init__()\n",
    "    \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.encoder = nn.LSTM(input_size=embedding_dim,  \n",
    "                hidden_size=hidden_size, num_layers=1, batch_first=True)\n",
    "        self.predictor = nn.Linear(hidden_size, num_class)\n",
    "\n",
    "    def forward(self, seq, offset):\n",
    "        embeds = self.embedding(seq)\n",
    "        lstm_out, (hn, cn) = self.encoder(embeds)\n",
    "        lstm_out = lstm_out[-1]\n",
    "        preds = self.predictor(lstm_out)\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тренируем модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_set[\"Категория\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OurFirstLSTM(\n",
      "  (embedding): Embedding(52487, 300)\n",
      "  (encoder): LSTM(300, 100, batch_first=True)\n",
      "  (predictor): Linear(in_features=100, out_features=22, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = OurFirstLSTM(100, 300, len(vocab), len(data_set[\"Категория\"].value_counts())).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextClassificationModel(\n",
      "  (embedding): EmbeddingBag(20000, 512, mode='mean')\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      "  (fc0): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (activ): ReLU()\n",
      "  (drop2): Dropout(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=256, out_features=15, bias=True)\n",
      ")\n",
      "vocab size:20000 embed_dim:512 num class:15\n"
     ]
    }
   ],
   "source": [
    "num_class = len(data_set[\"Категория\"].value_counts())\n",
    "vocab_size = len(vocab)\n",
    "emsize = 512\n",
    "model = TextClassificationModel(vocab_size, emsize, num_class).to(device)\n",
    "print(model)\n",
    "print(f\"vocab size:{vocab_size} embed_dim:{emsize} num class:{num_class}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Hyperparameters\n",
    "EPOCHS = 20  # epoch\n",
    "LR = 5  # learning rate\n",
    "BATCH_SIZE = 32  # batch size for training\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "\n",
    "def train(dataloader):\n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    log_interval = 500\n",
    "    start_time = time.time()\n",
    "\n",
    "    for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        predicted_label = model(text, offsets)\n",
    "        loss = criterion(predicted_label, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "        total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "        total_count += label.size(0)\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print(\n",
    "                \"| epoch {:3d} | {:5d}/{:5d} batches \"\n",
    "                \"| accuracy {:8.3f}\".format(\n",
    "                    epoch, idx, len(dataloader), total_acc / total_count\n",
    "                )\n",
    "            )\n",
    "            total_acc, total_count = 0, 0\n",
    "            start_time = time.time()\n",
    "\n",
    "\n",
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "            predicted_label = model(text, offsets)\n",
    "            loss = criterion(predicted_label, label)\n",
    "            total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    return total_acc / total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   500/ 1549 batches | accuracy    0.374\n",
      "| epoch   1 |  1000/ 1549 batches | accuracy    0.507\n",
      "| epoch   1 |  1500/ 1549 batches | accuracy    0.566\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time:  6.84s | valid accuracy    0.649 \n",
      "-----------------------------------------------------------\n",
      "| epoch   2 |   500/ 1549 batches | accuracy    0.603\n",
      "| epoch   2 |  1000/ 1549 batches | accuracy    0.624\n",
      "| epoch   2 |  1500/ 1549 batches | accuracy    0.633\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | time:  6.84s | valid accuracy    0.705 \n",
      "-----------------------------------------------------------\n",
      "| epoch   3 |   500/ 1549 batches | accuracy    0.666\n",
      "| epoch   3 |  1000/ 1549 batches | accuracy    0.676\n",
      "| epoch   3 |  1500/ 1549 batches | accuracy    0.670\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   3 | time:  6.68s | valid accuracy    0.711 \n",
      "-----------------------------------------------------------\n",
      "| epoch   4 |   500/ 1549 batches | accuracy    0.698\n",
      "| epoch   4 |  1000/ 1549 batches | accuracy    0.690\n",
      "| epoch   4 |  1500/ 1549 batches | accuracy    0.707\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   4 | time:  6.72s | valid accuracy    0.723 \n",
      "-----------------------------------------------------------\n",
      "| epoch   5 |   500/ 1549 batches | accuracy    0.718\n",
      "| epoch   5 |  1000/ 1549 batches | accuracy    0.718\n",
      "| epoch   5 |  1500/ 1549 batches | accuracy    0.724\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   5 | time:  6.76s | valid accuracy    0.735 \n",
      "-----------------------------------------------------------\n",
      "| epoch   6 |   500/ 1549 batches | accuracy    0.736\n",
      "| epoch   6 |  1000/ 1549 batches | accuracy    0.741\n",
      "| epoch   6 |  1500/ 1549 batches | accuracy    0.734\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   6 | time:  6.70s | valid accuracy    0.737 \n",
      "-----------------------------------------------------------\n",
      "| epoch   7 |   500/ 1549 batches | accuracy    0.755\n",
      "| epoch   7 |  1000/ 1549 batches | accuracy    0.757\n",
      "| epoch   7 |  1500/ 1549 batches | accuracy    0.750\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   7 | time:  6.71s | valid accuracy    0.738 \n",
      "-----------------------------------------------------------\n",
      "| epoch   8 |   500/ 1549 batches | accuracy    0.769\n",
      "| epoch   8 |  1000/ 1549 batches | accuracy    0.762\n",
      "| epoch   8 |  1500/ 1549 batches | accuracy    0.761\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   8 | time:  6.71s | valid accuracy    0.738 \n",
      "-----------------------------------------------------------\n",
      "| epoch   9 |   500/ 1549 batches | accuracy    0.774\n",
      "| epoch   9 |  1000/ 1549 batches | accuracy    0.775\n",
      "| epoch   9 |  1500/ 1549 batches | accuracy    0.773\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   9 | time:  6.72s | valid accuracy    0.751 \n",
      "-----------------------------------------------------------\n",
      "| epoch  10 |   500/ 1549 batches | accuracy    0.787\n",
      "| epoch  10 |  1000/ 1549 batches | accuracy    0.780\n",
      "| epoch  10 |  1500/ 1549 batches | accuracy    0.783\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  10 | time:  6.68s | valid accuracy    0.747 \n",
      "-----------------------------------------------------------\n",
      "| epoch  11 |   500/ 1549 batches | accuracy    0.814\n",
      "| epoch  11 |  1000/ 1549 batches | accuracy    0.814\n",
      "| epoch  11 |  1500/ 1549 batches | accuracy    0.819\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  11 | time:  6.70s | valid accuracy    0.755 \n",
      "-----------------------------------------------------------\n",
      "| epoch  12 |   500/ 1549 batches | accuracy    0.822\n",
      "| epoch  12 |  1000/ 1549 batches | accuracy    0.820\n",
      "| epoch  12 |  1500/ 1549 batches | accuracy    0.820\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  12 | time:  6.69s | valid accuracy    0.757 \n",
      "-----------------------------------------------------------\n",
      "| epoch  13 |   500/ 1549 batches | accuracy    0.823\n",
      "| epoch  13 |  1000/ 1549 batches | accuracy    0.825\n",
      "| epoch  13 |  1500/ 1549 batches | accuracy    0.826\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  13 | time:  6.68s | valid accuracy    0.760 \n",
      "-----------------------------------------------------------\n",
      "| epoch  14 |   500/ 1549 batches | accuracy    0.826\n",
      "| epoch  14 |  1000/ 1549 batches | accuracy    0.833\n",
      "| epoch  14 |  1500/ 1549 batches | accuracy    0.828\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  14 | time:  6.72s | valid accuracy    0.759 \n",
      "-----------------------------------------------------------\n",
      "| epoch  15 |   500/ 1549 batches | accuracy    0.833\n",
      "| epoch  15 |  1000/ 1549 batches | accuracy    0.835\n",
      "| epoch  15 |  1500/ 1549 batches | accuracy    0.828\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  15 | time:  6.69s | valid accuracy    0.761 \n",
      "-----------------------------------------------------------\n",
      "| epoch  16 |   500/ 1549 batches | accuracy    0.834\n",
      "| epoch  16 |  1000/ 1549 batches | accuracy    0.834\n",
      "| epoch  16 |  1500/ 1549 batches | accuracy    0.826\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  16 | time:  6.69s | valid accuracy    0.762 \n",
      "-----------------------------------------------------------\n",
      "| epoch  17 |   500/ 1549 batches | accuracy    0.834\n",
      "| epoch  17 |  1000/ 1549 batches | accuracy    0.833\n",
      "| epoch  17 |  1500/ 1549 batches | accuracy    0.830\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  17 | time:  6.70s | valid accuracy    0.761 \n",
      "-----------------------------------------------------------\n",
      "| epoch  18 |   500/ 1549 batches | accuracy    0.833\n",
      "| epoch  18 |  1000/ 1549 batches | accuracy    0.835\n",
      "| epoch  18 |  1500/ 1549 batches | accuracy    0.835\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  18 | time:  6.69s | valid accuracy    0.761 \n",
      "-----------------------------------------------------------\n",
      "| epoch  19 |   500/ 1549 batches | accuracy    0.836\n",
      "| epoch  19 |  1000/ 1549 batches | accuracy    0.834\n",
      "| epoch  19 |  1500/ 1549 batches | accuracy    0.834\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  19 | time:  6.71s | valid accuracy    0.761 \n",
      "-----------------------------------------------------------\n",
      "| epoch  20 |   500/ 1549 batches | accuracy    0.839\n",
      "| epoch  20 |  1000/ 1549 batches | accuracy    0.833\n",
      "| epoch  20 |  1500/ 1549 batches | accuracy    0.832\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  20 | time:  6.73s | valid accuracy    0.761 \n",
      "-----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "\n",
    "\n",
    "total_accu = None\n",
    "train_iter, test_iter = SupTextDataset(X_train[[\"Описание\", \"LableEncoder\"]], len(data_set[\"Категория\"].value_counts())),\\\n",
    "      SupTextDataset(X_test[[\"Описание\", \"LableEncoder\"]], len(data_set[\"Категория\"].value_counts()))\n",
    "train_dataset = to_map_style_dataset(train_iter)\n",
    "test_dataset = to_map_style_dataset(test_iter)\n",
    "num_train = int(len(train_dataset) * 0.95)\n",
    "split_train_, split_valid_ = random_split(\n",
    "    train_dataset, [num_train, len(train_dataset) - num_train]\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    split_train_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "valid_dataloader = DataLoader(\n",
    "    split_valid_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(train_dataloader)\n",
    "    accu_val = evaluate(valid_dataloader)\n",
    "    if total_accu is not None and total_accu > accu_val:\n",
    "        scheduler.step()\n",
    "    else:\n",
    "        total_accu = accu_val\n",
    "    print(\"-\" * 59)\n",
    "    print(\n",
    "        \"| end of epoch {:3d} | time: {:5.2f}s | \"\n",
    "        \"valid accuracy {:8.3f} \".format(\n",
    "            epoch, time.time() - epoch_start_time, accu_val\n",
    "        )\n",
    "    )\n",
    "    print(\"-\" * 59)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "c\n",
    "with open('../output/dict/classes_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(dict(zip([i for i in range(0,len(le.classes_))], le.classes_)), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(zip([i for i in range(0,len(le.classes_))], le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Checking the results of test dataset.\")\n",
    "accu_test = evaluate(test_dataloader)\n",
    "print(\"test accuracy {:8.3f}\".format(accu_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  3,  19, 243, 628,   0, 150,   0, 190])\n",
      "torch.return_types.max(\n",
      "values=tensor([0.3553]),\n",
      "indices=tensor([17]))\n",
      "torch.Size([1, 22])\n",
      "tensor([[ 3.2610,  0.2617,  0.7748, -4.5049,  2.2341,  7.8900, -1.5943, -4.5011,\n",
      "         -6.4304,  0.1897,  9.4881,  7.5935,  0.5631,  9.8114,  3.1982, -6.1653,\n",
      "          1.8838,  9.9004, -1.5470, -7.0061, -5.0645, -5.1897]])\n",
      "tensor(17)\n",
      "tensor([17])\n",
      "17\n",
      "tensor(9.9004)\n",
      "19\n",
      "tensor(-7.0061)\n",
      "Это удаленная тех. поддержка\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as nnf\n",
    "our_label = dict(zip([i for i in range(0,len(le.classes_))], le.classes_))\n",
    "\n",
    "\n",
    "def predict(text, text_pipeline):\n",
    "    with torch.no_grad():\n",
    "        text = torch.tensor(text_pipeline(text))\n",
    "        print(text)\n",
    "        output = model(text, torch.tensor([0]))\n",
    "        print(torch.max(nnf.softmax(output, dim=1), 1))\n",
    "        print(output.shape)\n",
    "        print(output)\n",
    "        print(output.argmax())\n",
    "        print(output.argmax(1))\n",
    "        print(output.argmax(1).item())\n",
    "        print(output[0][output.argmax(1).item()])\n",
    "        print(output.argmin(1).item())\n",
    "        print(output[0][output.argmin(1).item()])\n",
    "        return output.argmax(1).item()\n",
    "\n",
    "\n",
    "ex_text_str = cleanup_text(\"Прошу настроить портал Истока на компьютере . Очень срочно!\")\n",
    "\n",
    "model = model.to(\"cpu\")\n",
    "\n",
    "#print(\"Это %s\" % predict(ex_text_str, text_pipeline))\n",
    "print(\"Это %s\" % our_label[predict(ex_text_str, text_pipeline)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Протянуть витую пару новый компьютер'"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanup_text(\"Протянуть витую пару в новый компьютер\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28, 21])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(train_dataset[55][0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   9, 1868,  145,    6,  380])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 3\n",
    "num_epochs = 1\n",
    "train_dataset = SupTextDataset(X_train[[\"Описание\", \"Num\"]], len(data_set[\"Категория\"].value_counts()), vocab)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True, collate_fn=collate_batch)\n",
    "i = 0\n",
    "print(next(train_loader._get_iterator()))\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, lable in train_loader:\n",
    "        if i > 1:\n",
    "            continue\n",
    "        print(inputs,lable)\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56192"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train[:(X_train.shape[0]//32)*32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97159"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Почта не работает письма не отправляет   11\n"
     ]
    }
   ],
   "source": [
    "te1, te2 = train_dataset[1]\n",
    "print(te1, \" \", te2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56192"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"../output/modle.chek\")\n",
    "torch.save(vocab, '../output/vocab.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "with open('../output/dict/classes_dict.pkl', 'rb') as f:\n",
    "    loaded_dict = pickle.load(f)\n",
    "vocab = torch.load('../output/vocab.pth')\n",
    "num_class = len(loaded_dict)\n",
    "vocab_size = len(vocab)\n",
    "emsize = 512\n",
    "model = TextClassificationModel(vocab_size, emsize , 22)\n",
    "model_state_dict = torch.load(\"../output/modle.chek\")\n",
    "model.load_state_dict(model_state_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
